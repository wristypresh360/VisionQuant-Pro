# VisionQuant æ¶ˆèå®éªŒæ–¹æ¡ˆ

## ğŸ“‹ å®éªŒç›®æ ‡

ç³»ç»Ÿæ€§åœ°è¯„ä¼°VisionQuantå„ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼ŒéªŒè¯æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚

---

## ğŸ”¬ å®éªŒé…ç½®

### 1. Full Model (å®Œæ•´æ¨¡å‹)
- âœ… Self-Attention (8 heads)
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤ (threshold=0.5)
- âœ… æ—¶é—´éš”ç¦» (20å¤©)
- âœ… AttentionCAEç‰¹å¾æå–å™¨
- **é¢„æœŸ**: æœ€ä½³æ€§èƒ½

### 2. w/o Attention (ç§»é™¤æ³¨æ„åŠ›)
- âŒ Self-Attention
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âœ… æ—¶é—´éš”ç¦»
- âœ… åŸºç¡€CAEç‰¹å¾æå–å™¨
- **é¢„æœŸ**: Alphaä¸‹é™çº¦2-3%

### 3. w/o Correlation (ç§»é™¤ç›¸å…³æ€§è¿‡æ»¤)
- âœ… Self-Attention
- âŒ ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âœ… æ—¶é—´éš”ç¦»
- âœ… AttentionCAEç‰¹å¾æå–å™¨
- **é¢„æœŸ**: Alphaä¸‹é™çº¦5-7%ï¼ˆå› ä¸ºåŒ¹é…åˆ°ä¸ç›¸å…³çš„å½¢æ€ï¼‰

### 4. w/o Time Isolation (ç§»é™¤æ—¶é—´éš”ç¦»)
- âœ… Self-Attention
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âŒ æ—¶é—´éš”ç¦»
- âœ… AttentionCAEç‰¹å¾æå–å™¨
- **é¢„æœŸ**: Alphaä¸‹é™çº¦8-10%ï¼ˆå­˜åœ¨æ•°æ®æ³„æ¼é£é™©ï¼‰

### 5. ResNet Features (é¢„è®­ç»ƒResNet)
- âŒ Self-Attention
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âœ… æ—¶é—´éš”ç¦»
- âœ… ResNet50é¢„è®­ç»ƒç‰¹å¾
- **é¢„æœŸ**: Alphaä¸‹é™çº¦6-8%ï¼ˆé€šç”¨ç‰¹å¾vsä¸“ç”¨ç‰¹å¾ï¼‰

### 6. Heads=4 (4ä¸ªæ³¨æ„åŠ›å¤´)
- âœ… Self-Attention (4 heads)
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âœ… æ—¶é—´éš”ç¦»
- âœ… AttentionCAEç‰¹å¾æå–å™¨
- **é¢„æœŸ**: æ€§èƒ½ç•¥ä½äº8 heads

### 7. Heads=16 (16ä¸ªæ³¨æ„åŠ›å¤´)
- âœ… Self-Attention (16 heads)
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âœ… æ—¶é—´éš”ç¦»
- âœ… AttentionCAEç‰¹å¾æå–å™¨
- **é¢„æœŸ**: æ€§èƒ½å¯èƒ½è¿‡æ‹Ÿåˆï¼Œç•¥ä½äº8 heads

### 8. Dim=512 (ç‰¹å¾ç»´åº¦512)
- âœ… Self-Attention (8 heads)
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âœ… æ—¶é—´éš”ç¦»
- âœ… ç‰¹å¾ç»´åº¦512
- **é¢„æœŸ**: æ€§èƒ½ç•¥ä½äº1024ï¼ˆä¿¡æ¯æŸå¤±ï¼‰

### 9. Dim=2048 (ç‰¹å¾ç»´åº¦2048)
- âœ… Self-Attention (8 heads)
- âœ… ä»·æ ¼ç›¸å…³æ€§è¿‡æ»¤
- âœ… æ—¶é—´éš”ç¦»
- âœ… ç‰¹å¾ç»´åº¦2048
- **é¢„æœŸ**: æ€§èƒ½å¯èƒ½ç•¥å¥½ï¼Œä½†è®¡ç®—æˆæœ¬é«˜

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

å¯¹æ¯ä¸ªé…ç½®ï¼Œè®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š

1. **å¹³å‡æ”¶ç›Šç‡** (Average Return)
2. **Alpha** (ç›¸å¯¹äºBuy-and-Hold)
3. **å¤æ™®æ¯”ç‡** (Sharpe Ratio)
4. **æœ€å¤§å›æ’¤** (Max Drawdown)
5. **èƒœç‡** (Win Rate)
6. **äº¤æ˜“æ¬¡æ•°** (Number of Trades)

---

## ğŸ¯ å®éªŒæµç¨‹

### æ­¥éª¤1: å‡†å¤‡æ•°æ®
```bash
# ç¡®ä¿æœ‰æµ‹è¯•é›†æ•°æ®ï¼ˆ2023-07-01 åˆ° 2025-01-01ï¼‰
python -c "from src.data.data_loader import DataLoader; \
           loader = DataLoader(); \
           loader.get_stock_data('000001')"
```

### æ­¥éª¤2: è®­ç»ƒ/åŠ è½½æ¨¡å‹
```bash
# å¦‚æœè¿˜æ²¡æœ‰è®­ç»ƒAttentionCAEï¼Œå…ˆè®­ç»ƒ
python scripts/train_attention_cae.py
```

### æ­¥éª¤3: è¿è¡Œæ¶ˆèå®éªŒ
```bash
cd /Users/bytedance/PycharmProjects/pythonProject/VisionQuant-Pro
python src/strategies/ablation_study.py
```

### æ­¥éª¤4: æŸ¥çœ‹ç»“æœ
```bash
# ç»“æœä¿å­˜åœ¨ logs/ablation_results.csv
# LaTeXè¡¨æ ¼ä¿å­˜åœ¨ logs/ablation_table.tex
```

---

## ğŸ“ˆ é¢„æœŸç»“æœ

| é…ç½® | é¢„æœŸAlpha | é¢„æœŸSharpe | å…³é”®å‘ç° |
|------|-----------|------------|----------|
| Full Model | +14.8% | 1.92 | åŸºå‡† |
| w/o Attention | +12.3% | 1.78 | Attentionè´¡çŒ®+2.5% |
| w/o Correlation | +8.1% | 1.42 | ç›¸å…³æ€§è¿‡æ»¤è´¡çŒ®+6.7% |
| w/o Time Isolation | +5.2%* | 1.21 | æ—¶é—´éš”ç¦»é˜²æ­¢æ•°æ®æ³„æ¼ |
| ResNet Features | +7.4% | 1.38 | ä¸“ç”¨ç‰¹å¾ä¼˜äºé€šç”¨ç‰¹å¾ |
| Heads=4 | +13.5% | 1.85 | 8 headsæœ€ä¼˜ |
| Heads=16 | +13.2% | 1.82 | è¿‡æ‹Ÿåˆ |
| Dim=512 | +13.1% | 1.80 | 1024ç»´æœ€ä¼˜ |
| Dim=2048 | +14.9% | 1.93 | æå‡æœ‰é™ï¼Œæˆæœ¬é«˜ |

*æ³¨ï¼šw/o Time Isolationå¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼ï¼Œç»“æœä»…ä¾›å‚è€ƒ

---

## ğŸ” ç»Ÿè®¡åˆ†æ

å¯¹æ¯ä¸ªé…ç½®ï¼Œè¿›è¡Œï¼š

1. **é…å¯¹tæ£€éªŒ** (Paired t-test)
   - ä¸Full Modelå¯¹æ¯”
   - æ˜¾è‘—æ€§æ°´å¹³: p < 0.05

2. **æ•ˆåº”é‡** (Cohen's d)
   - è¯„ä¼°å®é™…å·®å¼‚å¤§å°
   - d > 0.8 ä¸ºå¤§æ•ˆåº”

3. **Wilcoxonç¬¦å·ç§©æ£€éªŒ**
   - éå‚æ•°æ£€éªŒï¼Œæ›´ç¨³å¥

---

## ğŸ“ ç»“æœè§£è¯»

### å…³é”®å‘ç°

1. **Self-Attentionçš„é‡è¦æ€§**
   - å¦‚æœw/o Attentionçš„Alphaä¸‹é™>2%ï¼Œè¯´æ˜Attentionæœ‰æ•ˆ
   - å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼ŒéªŒè¯æ¨¡å‹å…³æ³¨çš„å…³é”®åŒºåŸŸ

2. **ä»·æ ¼ç›¸å…³æ€§çš„å¿…è¦æ€§**
   - å¦‚æœw/o Correlationçš„Alphaå¤§å¹…ä¸‹é™ï¼Œè¯´æ˜çº¯è§†è§‰ç›¸ä¼¼åº¦ä¸å¤Ÿ
   - éœ€è¦ç»“åˆä»·æ ¼åŠ¨æ€

3. **æ—¶é—´éš”ç¦»çš„é˜²æ³„æ¼ä½œç”¨**
   - w/o Time Isolationçš„ç»“æœå¯èƒ½è™šé«˜ï¼ˆæ•°æ®æ³„æ¼ï¼‰
   - ä½†å®é™…Alphaåº”è¯¥æ›´ä½

4. **ä¸“ç”¨ç‰¹å¾vsé€šç”¨ç‰¹å¾**
   - ResNet Features vs AttentionCAE
   - éªŒè¯é¢†åŸŸä¸“ç”¨è®­ç»ƒçš„é‡è¦æ€§

---

## ğŸš€ è¿è¡Œç¤ºä¾‹

```python
from src.strategies.ablation_study import AblationStudy

# åˆ›å»ºå®éªŒå¯¹è±¡
study = AblationStudy()

# è¿è¡Œæ‰€æœ‰å®éªŒ
test_symbols = ['000001', '600519', '000858']  # æµ‹è¯•è‚¡ç¥¨
df_results = study.run_all_experiments(
    test_symbols=test_symbols,
    start_date="2023-07-01",
    end_date="2025-01-01"
)

# æŸ¥çœ‹ç»“æœ
print(df_results)

# ç”ŸæˆLaTeXè¡¨æ ¼
latex_table = study.generate_latex_table(df_results)
print(latex_table)
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°æ®ä¸€è‡´æ€§**: æ‰€æœ‰é…ç½®ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•é›†
2. **éšæœºç§å­**: å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
3. **è®¡ç®—èµ„æº**: å®Œæ•´å®éªŒå¯èƒ½éœ€è¦æ•°å°æ—¶
4. **æ¨¡å‹åŠ è½½**: ç¡®ä¿æ‰€æœ‰éœ€è¦çš„æ¨¡å‹æ–‡ä»¶éƒ½å­˜åœ¨

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- Ablation Studyæœ€ä½³å®è·µ
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒæ–¹æ³•
- é‡‘èå›æµ‹çš„æ³¨æ„äº‹é¡¹
