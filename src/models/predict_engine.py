import os
import sys
import torch
import numpy as np
import pandas as pd
import faiss
import glob
import gc
import csv
from tqdm import tqdm
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image

# === 1. åŸºç¡€ç¨³å¥é…ç½® ===
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '1'

# === 2. è·¯å¾„é…ç½® ===
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
DATA_RAW_DIR = os.path.join(PROJECT_ROOT, "data", "raw")
DATA_IMG_DIR = os.path.join(PROJECT_ROOT, "data", "images")
# è¿™é‡Œçš„æ¨¡å‹å…¶å®ç”¨ä¸åˆ°äº†(å› ä¸ºç‰¹å¾å·²ç»æå¥½äº†)ï¼Œä½†ä¸ºäº†å…¼å®¹ä¿ç•™
MODEL_PATH = os.path.join(PROJECT_ROOT, "data", "models", "cae_best.pth")

INDICES_DIR = os.path.join(PROJECT_ROOT, "data", "indices")
TEMP_DIR = os.path.join(INDICES_DIR, "temp_chunks")
os.makedirs(TEMP_DIR, exist_ok=True)

# å…³é”®æ–‡ä»¶è·¯å¾„
VECTORS_HUGE_MMAP = os.path.join(INDICES_DIR, "vectors_mmap.npy")  # 80GB æºæ–‡ä»¶
VECTORS_REDUCED_MMAP = os.path.join(INDICES_DIR, "vectors_reduced.npy")  # å‹ç¼©åæ–‡ä»¶
META_CSV_FILE = os.path.join(INDICES_DIR, "meta_data.csv")
INDEX_FILE = os.path.join(INDICES_DIR, "cae_faiss.bin")
PREDICTION_CACHE_FILE = os.path.join(INDICES_DIR, "prediction_cache.csv")

if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)
# åªéœ€è¦å¼•ç”¨ï¼Œä¸éœ€è¦å®ä¾‹åŒ–æ¨¡å‹
from src.models.autoencoder import QuantCAE


# å ä½ Dataset
class StockImageDataset(Dataset):
    def __init__(self, img_dir): pass

    def __len__(self): return 0

    def __getitem__(self, idx): return 0


class IndustrialPredictorReduced:
    def __init__(self):
        # é™ç»´æ“ä½œçº¯æ•°å­¦è®¡ç®—ï¼ŒCPU å¾ˆç¨³
        self.device = torch.device("cpu")
        print(f"ğŸ­ [é™ç»´å¼•æ“] å¯åŠ¨... ç›®æ ‡ç»´åº¦: 1024")
        self.returns_map = {}

    def run_pipeline(self):
        # Step 1: å‡†å¤‡æ”¶ç›Šç‡
        self._step1_prepare_returns()

        # Step 2: æ£€æŸ¥æºæ•°æ®
        if not os.path.exists(VECTORS_HUGE_MMAP) or not os.path.exists(META_CSV_FILE):
            print("âŒ ä¸¥é‡é”™è¯¯ï¼šæ‰¾ä¸åˆ° vectors_mmap.npy æˆ– meta_data.csvï¼")
            print("è¯·å…ˆè¿è¡Œä¹‹å‰çš„ [å·¥ä¸šå¼•æ“] ä»£ç å®Œæˆ Step 3ã€‚")
            return

        # è·å–æ•°æ®æ€»é‡
        df_meta = pd.read_csv(META_CSV_FILE, dtype=str)
        total_rows = len(df_meta)
        del df_meta
        gc.collect()

        print(f"ğŸ“Š æ£€æµ‹åˆ°æºæ•°æ®: {total_rows} æ¡è®°å½•")

        # Step 3.5: æ‰§è¡Œé™ç»´ (æ ¸å¿ƒï¼)
        self._step3_5_reduce_dimensions(total_rows)

        # Step 4: æ„å»ºç´¢å¼•
        self._step4_build_index(total_rows)

        # Step 5: é¢„æµ‹
        self._step5_batch_predict(total_rows)

    def _step1_prepare_returns(self):
        print("\n[Step 1] åŠ è½½æ”¶ç›Šç‡è¡¨...")
        csv_files = glob.glob(os.path.join(DATA_RAW_DIR, "*.csv"))
        for f in tqdm(csv_files, desc="Returns"):
            try:
                df = pd.read_csv(f, index_col=0, parse_dates=True)
                if len(df) < 5: continue
                symbol = os.path.basename(f).replace(".csv", "")
                future_close = df['Close'].shift(-5)
                ret = (future_close - df['Close']) / df['Close']
                for d, r in ret.items():
                    if not pd.isna(r):
                        self.returns_map[f"{symbol}_{d.strftime('%Y%m%d')}"] = r
            except:
                continue
        print(f"âœ… æ”¶ç›Šç‡åŠ è½½å®Œæˆ")

    def _step3_5_reduce_dimensions(self, total_rows):
        """å°† 50176 ç»´å‹ç¼©åˆ° 1024 ç»´"""
        # å¦‚æœå·²ç»å‹ç¼©è¿‡ï¼Œè·³è¿‡
        if os.path.exists(VECTORS_REDUCED_MMAP):
            # ç®€å•æ£€æŸ¥å¤§å°æ˜¯å¦åŒ¹é… (1024 * 4 bytes * rows)
            expected_size = total_rows * 1024 * 4
            if os.path.getsize(VECTORS_REDUCED_MMAP) >= expected_size:
                print("\n[Step 3.5] æ£€æµ‹åˆ°å·²é™ç»´æ–‡ä»¶ï¼Œè·³è¿‡å‹ç¼©ã€‚")
                return

        print(f"\n[Step 3.5] æ‰§è¡Œé«˜ç»´ç‰¹å¾å‹ç¼© (50176 -> 1024)...")
        print("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ª IO å¯†é›†å‹æ“ä½œï¼Œè¯·è€å¿ƒç­‰å¾…...")

        # 1. æ˜ å°„æºæ–‡ä»¶ (åªè¯»)
        huge_dim = 50176
        try:
            mmap_huge = np.memmap(VECTORS_HUGE_MMAP, dtype='float32', mode='r', shape=(total_rows, huge_dim))
        except:
            # è‡ªåŠ¨è®¡ç®—ç»´åº¦é˜²å´©
            file_size = os.path.getsize(VECTORS_HUGE_MMAP)
            huge_dim = file_size // (total_rows * 4)
            mmap_huge = np.memmap(VECTORS_HUGE_MMAP, dtype='float32', mode='r', shape=(total_rows, huge_dim))

        # 2. åˆ›å»ºç›®æ ‡æ–‡ä»¶
        target_dim = 1024
        mmap_small = np.memmap(VECTORS_REDUCED_MMAP, dtype='float32', mode='w+', shape=(total_rows, target_dim))

        # 3. å®šä¹‰æ± åŒ–å±‚ (è¿™æ˜¯é™ç»´çš„æ ¸å¿ƒæ•°å­¦å·¥å…·)
        # AdaptiveAvgPool1d ä¼šè‡ªåŠ¨æŠŠ 50176 ä¸ªæ•°å¹³å‡æˆ 1024 ä¸ªæ•°
        pool = torch.nn.AdaptiveAvgPool1d(target_dim)

        # 4. åˆ†æ‰¹å¤„ç†
        batch_size = 1000  # æ¯æ¬¡åªè¯» 1000 è¡Œï¼Œå†…å­˜å ç”¨æå° (~200MB)

        for i in tqdm(range(0, total_rows, batch_size), desc="Compressing"):
            end_i = min(i + batch_size, total_rows)

            # è¯»æ•°æ® (ä»ç¡¬ç›˜åŠ è½½åˆ°å†…å­˜)
            batch_huge = mmap_huge[i: end_i].copy()

            # è½¬ Tensor
            batch_tensor = torch.from_numpy(batch_huge).unsqueeze(1)  # [B, 1, 50176]

            # å‹ç¼©
            with torch.no_grad():
                batch_small = pool(batch_tensor).squeeze(1).numpy()

            # å†™å›ç¡¬ç›˜
            mmap_small[i: end_i] = batch_small

            # æ¸…ç†
            del batch_huge, batch_tensor, batch_small

        mmap_small.flush()
        print(f"âœ… å‹ç¼©å®Œæˆï¼ä½“ç§¯ç¼©å° 50 å€ã€‚")

    def _step4_build_index(self, total_rows):
        print("\n[Step 4] æ„å»º FAISS ç´¢å¼• (1024ç»´)...")

        # å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡
        if os.path.exists(INDEX_FILE):
            print("âœ… ç´¢å¼•æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return

        dim = 1024
        # è¯»å–å‹ç¼©åçš„æ•°æ®
        mmap_arr = np.memmap(VECTORS_REDUCED_MMAP, dtype='float32', mode='r', shape=(total_rows, dim))

        index = faiss.IndexFlatIP(dim)

        # åˆ†æ‰¹æ·»åŠ  (é˜²æ­¢ä¸€æ¬¡åŠ è½½ 1.6GB å¯¼è‡´ç¬é—´å¡é¡¿ï¼Œè™½ç„¶ 1.6GB å…¶å®è¿˜å¥½)
        batch_size = 50000
        for i in tqdm(range(0, total_rows, batch_size), desc="Indexing"):
            batch = mmap_arr[i: i + batch_size].copy()
            faiss.normalize_L2(batch)
            index.add(batch)
            del batch
            gc.collect()

        faiss.write_index(index, INDEX_FILE)
        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆã€‚")

    def _step5_batch_predict(self, total_rows):
        print("\n[Step 5] æµå¼æ¨æ¼” (åŸºäºå‹ç¼©ç‰¹å¾)...")

        if os.path.exists(PREDICTION_CACHE_FILE):
            os.remove(PREDICTION_CACHE_FILE)

        # åŠ è½½ç´¢å¼•
        index = faiss.read_index(INDEX_FILE)
        # åŠ è½½å‹ç¼©æ•°æ®
        mmap_arr = np.memmap(VECTORS_REDUCED_MMAP, dtype='float32', mode='r', shape=(total_rows, 1024))

        # åŠ è½½å…ƒæ•°æ®
        df_meta = pd.read_csv(META_CSV_FILE, dtype=str)
        meta_symbols = df_meta['symbol'].values
        meta_dates = df_meta['date'].values

        batch_size = 100  # æœç´¢æ‰¹æ¬¡

        with open(PREDICTION_CACHE_FILE, 'w') as f_out:
            f_out.write("symbol,date,pred_win_rate,pred_return,confidence\n")

            for start_idx in tqdm(range(0, total_rows, batch_size), desc="Predicting"):
                end_idx = min(start_idx + batch_size, total_rows)

                # ä»å‹ç¼©åçš„ mmap è¯»å– Query
                batch_vecs = mmap_arr[start_idx: end_idx].copy()
                faiss.normalize_L2(batch_vecs)

                # æé€Ÿæœç´¢
                D, I = index.search(batch_vecs, 20)

                lines = []
                for k in range(len(batch_vecs)):
                    current_idx = start_idx + k
                    curr_symbol = meta_symbols[current_idx]
                    curr_date = meta_dates[current_idx]

                    valid_ret = []
                    weights = []

                    for rank, neighbor_idx in enumerate(I[k]):
                        if neighbor_idx == current_idx: continue

                        nb_date = meta_dates[neighbor_idx]
                        if nb_date >= curr_date: continue  # æ—¶é—´é”

                        nb_symbol = meta_symbols[neighbor_idx]
                        key = f"{nb_symbol}_{nb_date}"

                        if key in self.returns_map:
                            valid_ret.append(self.returns_map[key])
                            weights.append(np.exp(D[k][rank] * 5))

                        if len(valid_ret) >= 10: break

                    if len(valid_ret) >= 3:
                        wr = sum(1 for r in valid_ret if r > 0) / len(valid_ret)
                        w = np.array(weights)
                        er = np.sum(np.array(valid_ret) * (w / w.sum()))
                        lines.append(f"{curr_symbol},{curr_date},{wr * 100:.2f},{er * 100:.2f},{len(valid_ret)}\n")

                f_out.writelines(lines)
                f_out.flush()

                if start_idx % 1000 == 0: gc.collect()

        print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {PREDICTION_CACHE_FILE}")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾› PredictEngine åˆ«å
PredictEngine = IndustrialPredictorReduced

if __name__ == "__main__":
    engine = IndustrialPredictorReduced()
    engine.run_pipeline()