import os
import sys
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import faiss
import pickle
import pandas as pd
import numpy as np
from datetime import datetime

# === 1. åŸºç¡€é…ç½® ===
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '1'

# === 2. è·¯å¾„é…ç½® ===
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
# ä¼˜å…ˆä½¿ç”¨ AttentionCAEï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ° QuantCAE
ATTENTION_MODEL_PATH = os.path.join(PROJECT_ROOT, "data", "models", "attention_cae_best.pth")
CAE_MODEL_PATH = os.path.join(PROJECT_ROOT, "data", "models", "cae_best.pth")
# ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆç”¨æ–°ç´¢å¼•ï¼‰
ATTENTION_INDEX_FILE = os.path.join(PROJECT_ROOT, "data", "indices", "cae_faiss_attention.bin")
ATTENTION_META_CSV = os.path.join(PROJECT_ROOT, "data", "indices", "meta_data_attention.csv")
INDEX_FILE = os.path.join(PROJECT_ROOT, "data", "indices", "cae_faiss.bin")
META_CSV = os.path.join(PROJECT_ROOT, "data", "indices", "meta_data.csv")
META_PKL = os.path.join(PROJECT_ROOT, "data", "indices", "meta.pkl")

if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)
from src.models.attention_cae import AttentionCAE


class VisionEngine:
    def __init__(self):
        self.device = torch.device("cpu")
        
        # 1. ä¼˜å…ˆåŠ è½½ AttentionCAEï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ° QuantCAE
        use_attention = os.path.exists(ATTENTION_MODEL_PATH)
        
        if use_attention:
            print(f"ğŸ‘ï¸ [VisionEngine] å¯åŠ¨ä¸­... åŠ è½½æ¨¡å‹: AttentionCAE")
            self.model = AttentionCAE(latent_dim=1024, num_attention_heads=8).to(self.device)
            try:
                state_dict = torch.load(ATTENTION_MODEL_PATH, map_location=self.device)
                self.model.load_state_dict(state_dict)
                self.model.eval()
                print(f"âœ… AttentionCAE åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ AttentionCAE æƒé‡åŠ è½½å¤±è´¥: {e}ï¼Œå›é€€åˆ° QuantCAE")
                use_attention = False
        
        if not use_attention:
            print(f"ğŸ‘ï¸ [VisionEngine] å¯åŠ¨ä¸­... åŠ è½½æ¨¡å‹: QuantCAE (å›é€€æ¨¡å¼)")
            from src.models.autoencoder import QuantCAE
            self.model = QuantCAE().to(self.device)
            if os.path.exists(CAE_MODEL_PATH):
                try:
                    state_dict = torch.load(CAE_MODEL_PATH, map_location=self.device)
                    self.model.load_state_dict(state_dict)
                    self.model.eval()
                    print(f"âœ… QuantCAE åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âŒ QuantCAE æƒé‡åŠ è½½å¤±è´¥: {e}")
        
        # QuantCAE éœ€è¦ pool é™ç»´ï¼ŒAttentionCAE å·²ç»è¾“å‡º 1024 ç»´
        self.use_attention = use_attention
        if not use_attention:
            self.pool = nn.AdaptiveAvgPool1d(1024)
        else:
            self.pool = None  # AttentionCAE ä¸éœ€è¦ pool

        self.preprocess = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
        ])

        self.index = None
        self.meta_data = []

    def reload_index(self):
        # ä¼˜å…ˆåŠ è½½ AttentionCAE ç´¢å¼•
        index_file = ATTENTION_INDEX_FILE if os.path.exists(ATTENTION_INDEX_FILE) else INDEX_FILE
        meta_file = ATTENTION_META_CSV if os.path.exists(ATTENTION_META_CSV) else META_CSV
        
        if not os.path.exists(index_file):
            print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
            return False

        print(f"ğŸ“¥ [VisionEngine] åŠ è½½ç´¢å¼•: {os.path.basename(index_file)}")
        try:
            self.index = faiss.read_index(index_file)
        except Exception as e:
            print(f"âŒ FAISS åŠ è½½å¤±è´¥: {e}")
            return False

        if os.path.exists(meta_file):
            df = pd.read_csv(meta_file, dtype=str)
            self.meta_data = df.to_dict('records')
        elif os.path.exists(META_PKL):
            with open(META_PKL, 'rb') as f:
                self.meta_data = pickle.load(f)
        else:
            print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {meta_file}")
            return False

        print(f"âœ… çŸ¥è¯†åº“å°±ç»ª: {len(self.meta_data)} æ¡è®°å½•")
        return True

    def _image_to_vector(self, img_path):
        try:
            img = Image.open(img_path).convert('RGB')
            input_tensor = self.preprocess(img).unsqueeze(0).to(self.device)
            with torch.no_grad():
                if self.use_attention:
                    # AttentionCAE.encode() å·²ç»è¿”å› 1024 ç»´çš„ L2 å½’ä¸€åŒ–å‘é‡
                    feature = self.model.encode(input_tensor)
                    return feature.cpu().numpy().flatten()
                else:
                    # QuantCAE.encode() è¿”å› 50176 ç»´ï¼Œéœ€è¦ pool é™ç»´
                    full_feature = self.model.encode(input_tensor)
                    reduced_feature = self.pool(full_feature.unsqueeze(1)).squeeze(1)
                    return reduced_feature.cpu().numpy().flatten()
        except:
            return None

    def search_similar_patterns(self, target_img_path, top_k=10, query_prices=None):
        """
        æ··åˆæœç´¢ï¼šè§†è§‰ç‰¹å¾ + ä»·æ ¼åºåˆ—ç›¸å…³æ€§
        
        Args:
            target_img_path: æŸ¥è¯¢Kçº¿å›¾è·¯å¾„
            top_k: è¿”å›Top-Kç»“æœ
            query_prices: æŸ¥è¯¢çš„ä»·æ ¼åºåˆ—ï¼ˆ20å¤©æ”¶ç›˜ä»·ï¼‰ï¼Œç”¨äºè®¡ç®—ç›¸å…³æ€§
        """
        if self.index is None:
            if not self.reload_index(): return []

        vec = self._image_to_vector(target_img_path)
        if vec is None: return []

        vec = vec.astype('float32').reshape(1, -1)
        faiss.normalize_L2(vec)

        # === ä¼˜åŒ–1: æ‰©å¤§æœç´¢èŒƒå›´ï¼Œè·å–æ›´å¤šå€™é€‰ ===
        search_k = max(top_k * 10, 200)  # ä»200ä¸ªå€™é€‰ä¸­ç­›é€‰
        D, I = self.index.search(vec, search_k)

        candidates = []
        seen_dates = {}
        ISOLATION_DAYS = 20

        # === ä¼˜åŒ–2: è§†è§‰å€™é€‰ +ï¼ˆå¯é€‰ï¼‰ä»·æ ¼ç›¸å…³æ€§ ===
        # æ³¨æ„ï¼šå¯¹â€œéçƒ­é—¨è‚¡/å†·é—¨æ—¥æœŸâ€ï¼Œåœ¨å¾ªç¯é‡Œé¢‘ç¹æ‹‰å–å†å²æ•°æ®å¾ˆå®¹æ˜“å¤±è´¥ã€‚
        # æˆ‘ä»¬å°†ç›¸å…³æ€§è§†ä¸ºâ€œå¯é€‰å¢å¼ºâ€ï¼šç®—å¾—å‡ºæ¥å°±æå‡æ’åºï¼Œç®—ä¸å‡ºæ¥å°±å›é€€åˆ°çº¯è§†è§‰TopKï¼Œ
        # è¿™æ ·æ‰èƒ½ä¿è¯å¯¹æ¯”å›¾å‡ ä¹ä¸å¯èƒ½ç©ºã€‚
        loader = None
        price_df_cache = {}
        if query_prices is not None and len(query_prices) == 20:
            try:
                from src.data.data_loader import DataLoader
                loader = DataLoader()
            except Exception:
                loader = None

        for vector_score, idx in zip(D[0], I[0]):
            if idx >= len(self.meta_data): continue

            info = self.meta_data[idx]
            sym = str(info['symbol']).zfill(6)
            date_str = str(info['date'])

            try:
                current_dt = datetime.strptime(date_str, "%Y%m%d")
            except:
                try:
                    current_dt = datetime.strptime(date_str, "%Y-%m-%d")
                except:
                    continue

            # æ—¶é—´éš”ç¦»æ£€æŸ¥
            is_conflict = False
            if sym in seen_dates:
                for existing_dt in seen_dates[sym]:
                    if abs((current_dt - existing_dt).days) < ISOLATION_DAYS:
                        is_conflict = True
                        break
            if is_conflict:
                continue

            # === ä¼˜åŒ–3: è®¡ç®—ä»·æ ¼åºåˆ—ç›¸å…³æ€§ï¼ˆå¯é€‰ï¼‰===
            correlation = None
            if loader is not None:
                try:
                    if sym not in price_df_cache:
                        dfp = loader.get_stock_data(sym)
                        if dfp is None or dfp.empty:
                            price_df_cache[sym] = None
                        else:
                            dfp.index = pd.to_datetime(dfp.index)
                            price_df_cache[sym] = dfp
                    else:
                        dfp = price_df_cache[sym]

                    if dfp is not None and (current_dt in dfp.index):
                        loc = dfp.index.get_loc(current_dt)
                        if loc >= 19:
                            match_prices = dfp.iloc[loc - 19: loc + 1]['Close'].values
                            query_norm = (query_prices - query_prices.mean()) / (query_prices.std() + 1e-8)
                            match_norm = (match_prices - match_prices.mean()) / (match_prices.std() + 1e-8)
                            corr = np.corrcoef(query_norm, match_norm)[0, 1]
                            if not np.isnan(corr):
                                correlation = float(corr)
                except Exception:
                    correlation = None

            # === ä¼˜åŒ–4: è¯„åˆ†ç­–ç•¥ï¼ˆä¿è¯ä¸ç©ºï¼‰===
            # ç›¸å…³æ€§ç®—ä¸å‡ºæ¥ï¼šé€€å›çº¯è§†è§‰ç›¸ä¼¼åº¦
            if correlation is None:
                final_score = float(vector_score)
            else:
                # ç›¸å…³æ€§ä½œä¸ºå¢å¼ºé¡¹ï¼Œæé«˜æ’åºç¨³å®šæ€§ï¼ˆä½†ä¸ä½œä¸ºç¡¬è¿‡æ»¤æ¡ä»¶ï¼‰
                final_score = 0.3 * float(vector_score) + 0.7 * float(correlation)

            candidates.append({
                "symbol": sym,
                "date": date_str,
                "score": float(final_score),
                "vector_score": float(vector_score),
                "correlation": (None if correlation is None else float(correlation))
            })

            seen_dates.setdefault(sym, []).append(current_dt)

        # === ä¼˜åŒ–6: æ’åºå¹¶è¿”å›ï¼ˆä¿è¯Top-Kï¼‰ ===
        candidates.sort(key=lambda x: x['score'], reverse=True)

        # è¿”å›Top-K
        return candidates[:top_k]


if __name__ == "__main__":
    if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)
    v = VisionEngine()
    v.reload_index()
    print("Vision Engine Ready")