import yfinance as yf
import requests
import xml.etree.ElementTree as ET
import datetime
import json
import re
import time
from collections import OrderedDict


class NewsHarvester:
    def __init__(self):
        # ä¼ªè£…æµè§ˆå™¨å¤´ï¼Œé˜²æ­¢ Google RSS åçˆ¬
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        # ç®€å•ç¼“å­˜ï¼Œé¿å…é‡å¤æ‹‰å–å¯¼è‡´å¡é¡¿
        self._cache = OrderedDict()
        self._cache_ttl = 600  # ç§’
        self._cache_max = 256

    def _cache_get(self, key):
        item = self._cache.get(key)
        if not item:
            return None
        if time.time() - item["ts"] > self._cache_ttl:
            self._cache.pop(key, None)
            return None
        self._cache.move_to_end(key)
        return item["data"]

    def _cache_set(self, key, data):
        self._cache[key] = {"ts": time.time(), "data": data}
        self._cache.move_to_end(key)
        if len(self._cache) > self._cache_max:
            self._cache.popitem(last=False)

    def _fetch_eastmoney_news(self, keyword, top_n=5, max_retries=3):
        """
        å·¥ä¸šçº§ä¼˜åŒ–ï¼šæ·»åŠ é‡è¯•æœºåˆ¶å’Œè¶…æ—¶æ§åˆ¶
        """
        url = "https://search-api-web.eastmoney.com/search/jsonp"
        callback = f"jQuery{int(time.time() * 1000)}"
        inner_param = {
            "uid": "",
            "keyword": keyword,
            "type": ["cmsArticleWebOld"],
            "client": "web",
            "clientType": "web",
            "clientVersion": "curr",
            "param": {
                "cmsArticleWebOld": {
                    "searchScope": "default",
                    "sort": "default",
                    "pageIndex": 1,
                    "pageSize": max(top_n, 10),
                    "preTag": "<em>",
                    "postTag": "</em>"
                }
            }
        }
        params = {
            "cb": callback,
            "param": json.dumps(inner_param, ensure_ascii=False),
            "_": str(int(time.time() * 1000))
        }
        headers = dict(self.headers)
        headers["referer"] = f"https://so.eastmoney.com/news/s?keyword={keyword}"
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                resp = requests.get(url, params=params, headers=headers, timeout=4)  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
                if resp.status_code != 200:
                    if attempt < max_retries - 1:
                        time.sleep(0.5 * (attempt + 1))  # æŒ‡æ•°é€€é¿
                        continue
                    return []
                text = resp.text.strip()
                match = re.search(r"\((\{.*\})\)\s*$", text, re.S)
                data_json = None
                if match:
                    data_json = json.loads(match.group(1))
                elif text.startswith("{") and text.endswith("}"):
                    data_json = json.loads(text)
                if not data_json:
                    if attempt < max_retries - 1:
                        continue
                    return []
                items = data_json.get("result", {}).get("cmsArticleWebOld", []) or []
                news_items = []
                for item in items[:top_n]:
                    title = str(item.get("title", "")).strip()
                    title = re.sub(r"</?em>", "", title)
                    date = str(item.get("date", ""))[:10] or "è¿‘æœŸ"
                    media = str(item.get("mediaName", "")).strip() or "ä¸œæ–¹è´¢å¯Œ"
                    if title:
                        news_items.append(f"- **{date}** ({media}) {title}")
                return news_items
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    time.sleep(0.5 * (attempt + 1))
                    continue
                return []
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(0.5 * (attempt + 1))
                    continue
                return []
        return []

    def get_latest_news(self, symbol, top_n=5):
        """
        [ä¸‰å¼•æ“å®¹é”™ç‰ˆ] è·å–æ–°é—»
        ä¼˜å…ˆçº§: AkShare -> Google RSS -> Yahoo Finance
        """
        symbol = str(symbol).strip().zfill(6)
        print(f"ğŸ“° [æ–°é—»ç›‘æ§] æ­£åœ¨æ‰«æ {symbol} çš„èˆ†æƒ…...")

        cache_key = f"{symbol}:{top_n}"
        cached = self._cache_get(cache_key)
        if cached:
            return cached

        news_items = []

        # === 1. ä¸œæ–¹è´¢å¯Œæ–°é—»æœç´¢ï¼ˆç¨³å¥ JSONP è§£æï¼‰ ===
        news_items = self._fetch_eastmoney_news(symbol, top_n=top_n)
        if not news_items:
            news_items = self._fetch_eastmoney_news(f"{symbol} è‚¡ç¥¨", top_n=top_n)
        if news_items:
            print("âœ… [æº:ä¸œæ–¹è´¢å¯Œ] è·å–æˆåŠŸ")
            result = "\n\n".join(news_items)
            self._cache_set(cache_key, result)
            return result

        # === 2. å°è¯• Google News RSS (å›½é™…æºï¼Œæœ€ç¨³) ===
        for attempt in range(2):  # æœ€å¤šé‡è¯•2æ¬¡
            try:
                query = f"{symbol} è‚¡ç¥¨"
                rss_url = f"https://news.google.com/rss/search?q={query}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
                response = requests.get(rss_url, headers=self.headers, timeout=4)  # ç¼©çŸ­è¶…æ—¶

                if response.status_code == 200:
                    root = ET.fromstring(response.content)
                    count = 0
                    for item in root.findall('./channel/item'):
                        if count >= top_n: break
                        title = item.find('title').text.split(' - ')[0]
                        pub_date = item.find('pubDate').text
                        try:
                            dt = datetime.datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                            date_str = dt.strftime("%Y-%m-%d")
                        except:
                            date_str = "è¿‘æœŸ"

                        news_items.append(f"- **{date_str}** (Google) {title}")
                        count += 1

                    if news_items:
                        print("âœ… [æº:Google News] è·å–æˆåŠŸ")
                        result = "\n\n".join(news_items)
                        self._cache_set(cache_key, result)
                        return result
                if attempt < 1:
                    time.sleep(0.5)
            except requests.exceptions.Timeout:
                if attempt < 1:
                    time.sleep(0.5)
                    continue
            except Exception as e:
                if attempt < 1:
                    time.sleep(0.5)
                    continue
                print(f"âŒ Google RSS å¼‚å¸¸: {e}")

        # === 3. å°è¯• Yahoo Finance (æœ€åé˜²çº¿) ===
        for attempt in range(2):  # æœ€å¤šé‡è¯•2æ¬¡
            try:
                yf_symbol = f"{symbol}.SS" if symbol.startswith('6') else f"{symbol}.SZ"
                yf_ticker = yf.Ticker(yf_symbol)
                yf_news = yf_ticker.news
                if yf_news:
                    for item in yf_news[:top_n]:
                        title = item.get('title')
                        ts = item.get('providerPublishTime')
                        if title and ts:
                            date_str = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d')
                            news_items.append(f"- **{date_str}** (Yahoo) {title}")

                    if news_items:
                        print("âœ… [æº:Yahoo] è·å–æˆåŠŸ")
                        result = "\n\n".join(news_items)
                        self._cache_set(cache_key, result)
                        return result
                if attempt < 1:
                    time.sleep(0.5)
            except Exception as e:
                if attempt < 1:
                    time.sleep(0.5)
                    continue
                pass

        result = "âœ… æš‚æ— é‡å¤§æ•æ„Ÿèˆ†æƒ… (å¤šæºæ‰«æå®Œæˆ)ã€‚"
        self._cache_set(cache_key, result)
        return result


if __name__ == "__main__":
    nh = NewsHarvester()
    print(nh.get_latest_news("600519"))