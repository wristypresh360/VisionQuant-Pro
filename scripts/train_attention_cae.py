"""
AttentionCAE è®­ç»ƒè„šæœ¬

è®­ç»ƒå¸¦Self-Attentionçš„å·ç§¯è‡ªç¼–ç å™¨
ä½¿ç”¨40ä¸‡å¼ Kçº¿å›¾è¿›è¡Œæ— ç›‘ç£å­¦ä¹ 

Author: Yisheng Pan
Date: 2026-01
"""

import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

# ç›´æ¥å¯¼å…¥ï¼Œé¿å… __init__.py çš„å¯¼å…¥é—®é¢˜
import sys
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src', 'models'))
from attention_cae import AttentionCAE


class KLineImageDataset(Dataset):
    """Kçº¿å›¾æ•°æ®é›†"""
    
    def __init__(self, image_dir, transform=None, max_samples=None):
        self.image_dir = image_dir
        self.transform = transform
        
        # è·å–æ‰€æœ‰PNGæ–‡ä»¶
        print(f"ğŸ“‚ æ‰«æå›¾ç‰‡ç›®å½•: {image_dir}")
        self.image_files = []
        
        for f in os.listdir(image_dir):
            if f.endswith('.png'):
                self.image_files.append(os.path.join(image_dir, f))
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¯é€‰ï¼‰
        if max_samples and len(self.image_files) > max_samples:
            np.random.seed(42)
            indices = np.random.choice(len(self.image_files), max_samples, replace=False)
            self.image_files = [self.image_files[i] for i in indices]
        
        print(f"âœ… æ‰¾åˆ° {len(self.image_files)} å¼ å›¾ç‰‡")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image, 0  # è¿”å›å›¾åƒå’Œå‡æ ‡ç­¾ï¼ˆæ— ç›‘ç£å­¦ä¹ ä¸éœ€è¦æ ‡ç­¾ï¼‰
        except Exception as e:
            # å¦‚æœå›¾ç‰‡æŸåï¼Œè¿”å›éšæœºå™ªå£°
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾ç‰‡ {img_path}: {e}")
            return torch.randn(3, 224, 224), 0


def train_attention_cae(
    image_dir: str,
    output_dir: str,
    epochs: int = 5,
    batch_size: int = 32,
    learning_rate: float = 1e-3,
    max_samples: int = None,
    use_attention: bool = True
):
    """
    è®­ç»ƒ AttentionCAE æ¨¡å‹
    
    Args:
        image_dir: Kçº¿å›¾ç›®å½•
        output_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹å¤§å°
        learning_rate: å­¦ä¹ ç‡
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        use_attention: æ˜¯å¦ä½¿ç”¨Attentionæ¨¡å—
    """
    
    # è®¾å¤‡é€‰æ‹©
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print("ğŸš€ ä½¿ç”¨ CUDA GPU åŠ é€Ÿ")
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("ğŸš€ ä½¿ç”¨ Apple MPS GPU åŠ é€Ÿ")
    else:
        device = torch.device('cpu')
        print("âš ï¸ ä½¿ç”¨ CPUï¼ˆè¾ƒæ…¢ï¼‰")
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = KLineImageDataset(image_dir, transform, max_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {train_size} å¼ , éªŒè¯é›†: {val_size} å¼ ")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    # åˆ›å»ºæ¨¡å‹
    model_name = "AttentionCAE" if use_attention else "CAE"
    print(f"\nğŸ§  åˆ›å»ºæ¨¡å‹: {model_name}")
    model = AttentionCAE(latent_dim=1024, num_attention_heads=8, use_attention=use_attention)
    model = model.to(device)
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“ æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
    criterion = nn.MSELoss()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸƒ å¼€å§‹è®­ç»ƒ ({epochs} epochs)")
    print("=" * 60)
    
    best_val_loss = float('inf')
    
    for epoch in range(1, epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs} [Train]")
        for batch_idx, (images, _) in enumerate(pbar):
            images = images.to(device)
            
            # å‰å‘ä¼ æ’­
            recon, latent = model(images)
            loss = criterion(recon, images)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.6f}'})
        
        train_loss /= len(train_loader)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for images, _ in tqdm(val_loader, desc=f"Epoch {epoch}/{epochs} [Val]"):
                images = images.to(device)
                recon, _ = model(images)
                loss = criterion(recon, images)
                val_loss += loss.item()
        
        val_loss /= len(val_loader)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“ˆ Epoch {epoch}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, LR = {current_lr:.2e}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(output_dir, f"attention_cae_epoch_{epoch}.pth")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
        }, checkpoint_path)
        print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_path = os.path.join(output_dir, "attention_cae_best.pth")
            torch.save(model.state_dict(), best_path)
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path} (Val Loss: {val_loss:.6f})")
        
        print("-" * 60)
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")
    
    return model


if __name__ == "__main__":
    # é…ç½®
    IMAGE_DIR = os.path.join(PROJECT_ROOT, "data", "images")
    OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "models")
    
    # å¼€å§‹è®­ç»ƒ
    print("=" * 60)
    print("ğŸ¯ AttentionCAE è®­ç»ƒè„šæœ¬")
    print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    model = train_attention_cae(
        image_dir=IMAGE_DIR,
        output_dir=OUTPUT_DIR,
        epochs=5,              # 5è½®è®­ç»ƒ
        batch_size=32,         # æ‰¹å¤§å°
        learning_rate=1e-3,    # å­¦ä¹ ç‡
        max_samples=None,      # ä½¿ç”¨å…¨éƒ¨æ•°æ®
        use_attention=True     # ä½¿ç”¨Attention
    )
    
    print(f"\nğŸ“… ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
