"""
Train Dual-Stream Network - åŒæµç½‘ç»œè®­ç»ƒè„šæœ¬

å®ç°å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼š
1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
2. Walk-Forwardäº¤å‰éªŒè¯
3. å¤šä»»åŠ¡å­¦ä¹ ï¼ˆåˆ†ç±»+å›å½’ï¼‰
4. æ¨¡å‹ä¿å­˜å’Œè¯„ä¼°

Author: VisionQuant Team
Date: 2026-01
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
import argparse
import json
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
sys.path.insert(0, PROJECT_ROOT)

from src.models.dual_stream_network import DualStreamNetwork, DualStreamLoss
from src.data.gaf_encoder import GAFEncoder
from src.data.triple_barrier import TripleBarrierLabeler
from src.utils.walk_forward import WalkForwardValidator, TimeSeriesSplitter


class DualStreamDataset(Dataset):
    """
    åŒæµç½‘ç»œæ•°æ®é›†
    
    åŒæ—¶æä¾›GAFå›¾åƒå’ŒOHLCVåºåˆ—
    """
    
    def __init__(
        self,
        data_df: pd.DataFrame,
        gaf_dir: str,
        window_size: int = 60,
        transform=None
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_df: åŒ…å«æ ‡ç­¾å’Œå…ƒæ•°æ®çš„DataFrame
            gaf_dir: GAFå›¾åƒç›®å½•
            window_size: OHLCVçª—å£å¤§å°
            transform: å›¾åƒå˜æ¢
        """
        self.data_df = data_df.reset_index(drop=True)
        self.gaf_dir = gaf_dir
        self.window_size = window_size
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
        
        # é¢„åŠ è½½OHLCVæ•°æ®ï¼ˆé¿å…é‡å¤è¯»å–ï¼‰
        self.ohlcv_cache = {}
    
    def __len__(self):
        return len(self.data_df)
    
    def __getitem__(self, idx):
        row = self.data_df.iloc[idx]
        
        # è·å–å…ƒæ•°æ®
        symbol = str(row['symbol'])
        date_str = str(row['date'])
        
        # åŠ è½½GAFå›¾åƒ
        gaf_path = os.path.join(self.gaf_dir, f"{symbol}_{date_str}.png")
        if os.path.exists(gaf_path):
            gaf_image = Image.open(gaf_path).convert('RGB')
            gaf_image = self.transform(gaf_image)
        else:
            # ä½¿ç”¨ç©ºç™½å›¾åƒä½œä¸ºå ä½ç¬¦
            gaf_image = torch.zeros(3, 224, 224)
        
        # è·å–OHLCVåºåˆ—
        if 'ohlcv' in row:
            ohlcv = np.array(row['ohlcv'])
        else:
            # ä»ç¼“å­˜æˆ–æ–‡ä»¶åŠ è½½
            ohlcv = self._load_ohlcv(symbol, date_str)
        
        # ç¡®ä¿OHLCVå½¢çŠ¶æ­£ç¡®
        if ohlcv.shape[0] < self.window_size:
            # å¡«å……
            pad_size = self.window_size - ohlcv.shape[0]
            ohlcv = np.pad(ohlcv, ((pad_size, 0), (0, 0)), mode='edge')
        elif ohlcv.shape[0] > self.window_size:
            ohlcv = ohlcv[-self.window_size:]
        
        ohlcv_tensor = torch.FloatTensor(ohlcv)
        
        # æ ‡ç­¾
        class_label = int(row.get('tb_label', 0)) + 1  # -1,0,1 -> 0,1,2
        return_label = float(row.get('tb_return', 0))
        
        return {
            'gaf_image': gaf_image,
            'ohlcv': ohlcv_tensor,
            'class_label': class_label,
            'return_label': return_label,
            'symbol': symbol,
            'date': date_str
        }
    
    def _load_ohlcv(self, symbol: str, date_str: str) -> np.ndarray:
        """åŠ è½½OHLCVæ•°æ®"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®å­˜å‚¨æ–¹å¼å®ç°
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›éšæœºæ•°æ®
        return np.random.randn(self.window_size, 5).astype(np.float32)


class DualStreamTrainer:
    """
    åŒæµç½‘ç»œè®­ç»ƒå™¨
    """
    
    def __init__(
        self,
        model: DualStreamNetwork,
        device: torch.device,
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-5
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: åŒæµç½‘ç»œæ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
        """
        self.model = model.to(device)
        self.device = device
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=100,
            eta_min=1e-6
        )
        
        # æŸå¤±å‡½æ•°
        self.loss_fn = DualStreamLoss(
            class_weight=1.0,
            return_weight=0.5
        )
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def train_epoch(self, train_loader: DataLoader) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0
        total_class_loss = 0
        total_return_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc='Training')
        for batch in pbar:
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            gaf_image = batch['gaf_image'].to(self.device)
            ohlcv = batch['ohlcv'].to(self.device)
            class_label = batch['class_label'].to(self.device)
            return_label = batch['return_label'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(gaf_image, ohlcv)
            
            # è®¡ç®—æŸå¤±
            losses = self.loss_fn(outputs, class_label, return_label)
            
            # åå‘ä¼ æ’­
            losses['total_loss'].backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += losses['total_loss'].item()
            total_class_loss += losses['class_loss'].item()
            total_return_loss += losses['return_loss'].item()
            
            pred = torch.argmax(outputs['class_logits'], dim=-1)
            correct += (pred == class_label).sum().item()
            total += class_label.size(0)
            
            pbar.set_postfix({
                'loss': f"{losses['total_loss'].item():.4f}",
                'acc': f"{correct/total:.4f}"
            })
        
        n_batches = len(train_loader)
        return {
            'loss': total_loss / n_batches,
            'class_loss': total_class_loss / n_batches,
            'return_loss': total_return_loss / n_batches,
            'accuracy': correct / total
        }
    
    @torch.no_grad()
    def validate(self, val_loader: DataLoader) -> dict:
        """éªŒè¯"""
        self.model.eval()
        
        total_loss = 0
        correct = 0
        total = 0
        
        all_preds = []
        all_labels = []
        
        for batch in tqdm(val_loader, desc='Validating'):
            gaf_image = batch['gaf_image'].to(self.device)
            ohlcv = batch['ohlcv'].to(self.device)
            class_label = batch['class_label'].to(self.device)
            return_label = batch['return_label'].to(self.device)
            
            outputs = self.model(gaf_image, ohlcv)
            losses = self.loss_fn(outputs, class_label, return_label)
            
            total_loss += losses['total_loss'].item()
            
            pred = torch.argmax(outputs['class_logits'], dim=-1)
            correct += (pred == class_label).sum().item()
            total += class_label.size(0)
            
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(class_label.cpu().numpy())
        
        n_batches = len(val_loader)
        return {
            'loss': total_loss / n_batches,
            'accuracy': correct / total,
            'predictions': np.array(all_preds),
            'labels': np.array(all_labels)
        }
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        num_epochs: int = 50,
        save_dir: str = None,
        patience: int = 10
    ) -> dict:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            patience: æ—©åœè€å¿ƒå€¼
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        best_val_loss = float('inf')
        best_epoch = 0
        no_improve = 0
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            print("-" * 50)
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)
            print(f"Train - Loss: {train_metrics['loss']:.4f}, "
                  f"Acc: {train_metrics['accuracy']:.4f}")
            
            # éªŒè¯
            val_metrics = self.validate(val_loader)
            print(f"Val   - Loss: {val_metrics['loss']:.4f}, "
                  f"Acc: {val_metrics['accuracy']:.4f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['train_acc'].append(train_metrics['accuracy'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['loss'] < best_val_loss:
                best_val_loss = val_metrics['loss']
                best_epoch = epoch
                no_improve = 0
                
                if save_dir:
                    self._save_checkpoint(save_dir, epoch, val_metrics)
                    print(f"âœ… Saved best model (epoch {epoch + 1})")
            else:
                no_improve += 1
            
            # æ—©åœ
            if no_improve >= patience:
                print(f"\nâš ï¸ Early stopping at epoch {epoch + 1}")
                break
        
        print(f"\nğŸ‰ Training completed! Best epoch: {best_epoch + 1}")
        
        return {
            'best_epoch': best_epoch,
            'best_val_loss': best_val_loss,
            'history': self.history
        }
    
    def _save_checkpoint(self, save_dir: str, epoch: int, metrics: dict):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        model_path = os.path.join(save_dir, 'dual_stream_best.pth')
        torch.save(self.model.state_dict(), model_path)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state_path = os.path.join(save_dir, 'training_state.json')
        state = {
            'epoch': epoch,
            'val_loss': metrics['loss'],
            'val_accuracy': metrics['accuracy'],
            'history': self.history
        }
        with open(state_path, 'w') as f:
            json.dump(state, f, indent=2)


def create_synthetic_dataset(
    n_samples: int = 1000,
    window_size: int = 60
) -> pd.DataFrame:
    """åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæµ‹è¯•"""
    np.random.seed(42)
    
    data = []
    for i in range(n_samples):
        symbol = f"{600000 + i % 100:06d}"
        date = f"2023{(i % 12) + 1:02d}{(i % 28) + 1:02d}"
        
        # éšæœºæ ‡ç­¾
        tb_label = np.random.choice([-1, 0, 1])
        tb_return = np.random.randn() * 0.05
        
        # éšæœºOHLCV
        ohlcv = np.random.randn(window_size, 5).astype(np.float32)
        
        data.append({
            'symbol': symbol,
            'date': date,
            'tb_label': tb_label,
            'tb_return': tb_return,
            'ohlcv': ohlcv.tolist()
        })
    
    return pd.DataFrame(data)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Train Dual-Stream Network')
    parser.add_argument('--data_dir', type=str, default='data',
                       help='æ•°æ®ç›®å½•')
    parser.add_argument('--gaf_dir', type=str, default='data/gaf_images',
                       help='GAFå›¾åƒç›®å½•')
    parser.add_argument('--save_dir', type=str, default='data/models',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=50,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--window_size', type=int, default=60,
                       help='OHLCVçª—å£å¤§å°')
    parser.add_argument('--use_synthetic', action='store_true',
                       help='ä½¿ç”¨åˆæˆæ•°æ®æµ‹è¯•')
    
    args = parser.parse_args()
    
    # è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print("ğŸš€ Using CUDA GPU")
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("ğŸš€ Using Apple MPS GPU")
    else:
        device = torch.device('cpu')
        print("ğŸ’» Using CPU")
    
    # åˆ›å»º/åŠ è½½æ•°æ®
    if args.use_synthetic:
        print("\nğŸ“¦ Creating synthetic dataset for testing...")
        data_df = create_synthetic_dataset(n_samples=1000)
    else:
        # åŠ è½½çœŸå®æ•°æ®
        data_path = os.path.join(args.data_dir, 'labeled_data.csv')
        if os.path.exists(data_path):
            data_df = pd.read_csv(data_path)
        else:
            print(f"âš ï¸ Data file not found: {data_path}")
            print("ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæµ‹è¯•...")
            data_df = create_synthetic_dataset(n_samples=1000)
    
    print(f"ğŸ“Š Dataset size: {len(data_df)}")
    
    # åˆ’åˆ†æ•°æ®
    train_df, val_df, test_df = TimeSeriesSplitter.train_test_split(
        data_df, test_size=0.2, val_size=0.1
    )
    
    print(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = DualStreamDataset(
        train_df, args.gaf_dir, args.window_size
    )
    val_dataset = DualStreamDataset(
        val_df, args.gaf_dir, args.window_size
    )
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=4 if device.type != 'mps' else 0,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4 if device.type != 'mps' else 0,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = DualStreamNetwork(
        vision_backbone='resnet18',
        vision_pretrained=True,
        vision_dim=512,
        temporal_input_size=5,
        temporal_dim=256,
        temporal_layers=2,
        fusion_dim=768,
        num_classes=3
    )
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ§  Model Parameters: {total_params:,} (trainable: {trainable_params:,})")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DualStreamTrainer(
        model=model,
        device=device,
        learning_rate=args.lr
    )
    
    # è®­ç»ƒ
    print("\nğŸƒ Starting training...")
    results = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=args.num_epochs,
        save_dir=args.save_dir,
        patience=10
    )
    
    print("\nâœ… Training completed!")
    print(f"Best Epoch: {results['best_epoch'] + 1}")
    print(f"Best Val Loss: {results['best_val_loss']:.4f}")


if __name__ == "__main__":
    main()
